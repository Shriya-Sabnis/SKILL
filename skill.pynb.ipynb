{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are importing the dataset and performing data cleaning making the dataset suatiable for futher analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\sabni\\Documents\\SKILL\\ObesityDataSet_raw_and_data_sinthetic.csv\")\n",
    "print(df.head(20))\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(axis=0, inplace=True)\n",
    "\n",
    "# Display info and basic stats\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Check for null values\n",
    "print(\"Any Null Values Present:\")\n",
    "print(df.isnull().any())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "print(\"Sum of Null Values per Column:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Any Duplicated Rows:\")\n",
    "print(df.duplicated().any())\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame\n",
    "output_path = r\"C:\\Users\\sabni\\Documents\\SKILL\\hi2.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are performing lable encoding by label encoder and one-hot-encoder\n",
    "lable encoding is the feature in ml which allows us to convert the values in the categorical columns into numbers so that the ML model can better understand them and use them in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Copy original dataset for encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Label Encoding for categorical columns\n",
    "label_encoders = {}\n",
    "for col in ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC','SMOKE', \n",
    "            'SCC', 'CALC', 'MTRANS','NObeyesdad']:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col + '_Label'] = le.fit_transform(df_encoded[col])\n",
    "    label_encoders[col] = le  # Store the encoder for reference\n",
    "\n",
    "# One-Hot Encoding for categorical columns\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(df[['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC' ,'SMOKE',\n",
    "            'SCC', 'CALC', 'MTRANS','NObeyesdad']])\n",
    "\n",
    "# Convert One-Hot Encoded array to DataFrame\n",
    "#one_hot_df = pd.DataFrame(one_hot_encoded, columns=one_hot_encoder.get_feature_names_out())\n",
    "\n",
    "# Concatenate One-Hot Encoded columns with original dataframe\n",
    "#df_encoded = pd.concat([df_encoded, one_hot_df], axis=1)\n",
    "df=df_encoded.copy()\n",
    "# Drop original categorical columns\n",
    "df.drop(columns=['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC','SMOKE', \n",
    "            'SCC', 'CALC', 'MTRANS','NObeyesdad'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the encoded dataset\n",
    "df.head(200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are performing normailazation anf standerdization\n",
    "Normalization (Min-Max Scaling): Rescales the data to a fixed range, typically [0,1] or [-1,1]. The formula is:\n",
    "Standardization (Z-score Scaling): Transforms data to have a mean of 0 and a standard deviation of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "# Identifying numerical columns (ensure correct column names)\n",
    "numerical_cols = ['Age','Height', 'Weight', 'FCVC', 'NCP', 'FAF', 'TUE']  # Removed 'Weight1'\n",
    "\n",
    "# Min-Max Scaling (Normalization: scales values between 0 and 1)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df[[col + '_MinMax' for col in numerical_cols]] = min_max_scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Standard Scaling (Z-score: mean = 0, std = 1)\n",
    "standard_scaler = StandardScaler()\n",
    "df[[col + '_ZScore' for col in numerical_cols]] = standard_scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Display first few rows after scaling\n",
    "print(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a histogram of the output distribution \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.histplot(df['NObeyesdad_Label'], kde=True, bins=30)\n",
    "plt.title('Output Distribution')\n",
    "plt.xlabel('NObeyesdad')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot 2: Histogram of Heart Rate with KDE\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.histplot(df['Age'], kde=True, bins=30)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot of weight1 with weight\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data =df, x=df['Weight'], y=df['Height'], hue=\"NObeyesdad_Label\",palette=\"coolwarm\")  # Scatter plot for two continuous variables\n",
    "plt.title('Weight1 vs Height')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Height')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 6: Bar plot of Age vs.NObeyesdad_Label\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Age', y=\"NObeyesdad_Label\", data=df)  # ci=None to avoid confidence intervals\n",
    "plt.title('Age vs.NObeyesdad_Label')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('NObeyesdad_Label')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 8: lineplot with marker of Age vs.NObeyesdad_Label\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=df, x=\"Age\",y=\"NObeyesdad_Label\", label=\"lineplot with marker\", color=\"blue\", marker=\"o\" )\n",
    "plt.title('Age vs NObeyesdad_Label')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('NObeyesdad_Label')\n",
    "plt.show()\n",
    "# Count the occurrences of each unique value in the 'Heart_Attack_Risk' column\n",
    "risk_counts = df['NObeyesdad_Label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(risk_counts, labels=risk_counts.index, autopct='%1.1f%%', startangle=90, colors=['lightblue', 'salmon','yellow'])\n",
    "plt.title('NObeyesdad_Label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are finding and removing outliers using the IQR and Z-score methods.  \n",
    "\n",
    "Outliers are data points that deviate from the normal distribution of the data. These may reduce the accuracy of the model; hence, they are detected and removed before the dataset is given to the model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Function to remove outliers using IQR\n",
    "def remove_outliers_iqr(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:  # Only process numerical columns\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            # Filter out outliers\n",
    "            df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to remove outliers\n",
    "df_no_outliers_iqr = remove_outliers_iqr(df)\n",
    "\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "print(\"\\nDataFrame after removing outliers:\")\n",
    "print(df_no_outliers_iqr.head(20))\n",
    "print(\"\\n\\n\\n\\n\")\n",
    "output_path = r\"C:/Users/sabni/Documents/SKILL/hi3.csv\"\n",
    "df_no_outliers_iqr.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Calculate Z-scores for each numeric column\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in ['int64', 'float64']:\n",
    "        z_scores = zscore(df[col])\n",
    "        print(f\"Z-scores for column '{col}':\")\n",
    "        print(z_scores)\n",
    "\n",
    "\n",
    "# Function to remove outliers using Z-score\n",
    "def remove_outliers_zscore(df, threshold=1):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['int64', 'float64']:  # Only process numerical columns\n",
    "            z_scores = zscore(df[col])\n",
    "            df = df[abs(z_scores) <= threshold]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function to remove outliers\n",
    "df_no_outliers_zscore = remove_outliers_zscore(df)\n",
    "\n",
    "output_path = r\"C:/Users/sabni/Documents/SKILL/hi4.csv\"\n",
    "df_no_outliers_zscore.to_csv(output_path, index=False)\n",
    "print(\"\\nDataFrame after removing outliers:\")\n",
    "print(df_no_outliers_zscore.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we are performing feature selection using chi square method and ANOVA \n",
    "ANOVA compares the means of three or more groups to check if they differ significantly. It analyzes the effect of categorical independent variables on a continuous dependent variable using the F-statistic.\n",
    "Chi-Square tests the association between two categorical variables by comparing observed and expected frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, chi2\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define feature sets\n",
    "numerical_features = [\"Age\", \"Height\", \"Weight1\", \"FCVC\", \"NCP\", \"FAF\", \"TUE\", \"Weight\"]\n",
    "categorical_features = [\"Gender_Label\", \"family_history_with_overweight_Label\", \"FAVC_Label\", \"CAEC_Label\", \"SCC_Label\", \"CALC_Label\", \"MTRANS_Label\"]\n",
    "\n",
    "y = df[\"NObeyesdad_Label\"]  # Using pre-encoded target variable\n",
    "\n",
    "# ANOVA F-test for numerical features\n",
    "X_numerical = df[numerical_features]\n",
    "anova_results = f_classif(X_numerical, y)\n",
    "anova_scores = pd.DataFrame({\"Feature\": numerical_features, \"ANOVA F-Value\": anova_results[0]})\n",
    "\n",
    "# Chi-square test for pre-encoded categorical features\n",
    "X_categorical = df[categorical_features]\n",
    "chi2_results = chi2(X_categorical, y)\n",
    "chi2_scores = pd.DataFrame({\"Feature\": categorical_features, \"Chi-Square Score\": chi2_results[0]})\n",
    "\n",
    "# Select features based on threshold values\n",
    "anova_threshold = 1.0  # Example threshold for ANOVA\n",
    "chi2_threshold = 1.0  # Example threshold for Chi-Square\n",
    "\n",
    "selected_numerical_features = anova_scores[anova_scores[\"ANOVA F-Value\"] > anova_threshold][\"Feature\"].tolist()\n",
    "selected_categorical_features = chi2_scores[chi2_scores[\"Chi-Square Score\"] > chi2_threshold][\"Feature\"].tolist()\n",
    "\n",
    "# Print results in tabular form\n",
    "print(\"ANOVA F-Test Scores (Numerical Features):\")\n",
    "print(anova_scores.to_string(index=False))\n",
    "print(\"\\nChi-Square Scores (Categorical Features):\")\n",
    "print(chi2_scores.to_string(index=False))\n",
    "\n",
    "# Print selected features\n",
    "print(\"\\nSelected Numerical Features based on ANOVA:\")\n",
    "print(selected_numerical_features)\n",
    "print(\"\\nSelected Categorical Features based on Chi-Square:\")\n",
    "print(selected_categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain (IG) is a feature selection technique used in machine learning to determine the relevance of a feature based on how much it reduces uncertainty (entropy) in the target variable.\n",
    "A higher Information Gain indicates a more important feature. It is widely used in decision trees and classification tasks to select the most informative attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Information Gain (Mutual Information) for all features\n",
    "X_all = df[numerical_features + categorical_features]\n",
    "info_gain_scores = mutual_info_classif(X_all, y)\n",
    "info_gain_df = pd.DataFrame({\"Feature\": numerical_features + categorical_features, \"Information Gain\": info_gain_scores})\n",
    "\n",
    "# Print Information Gain scores in tabular form\n",
    "print(\"\\nInformation Gain (Mutual Information) Scores:\")\n",
    "print(info_gain_df.to_string(index=False))\n",
    "# Select features with Information Gain above a threshold (e.g., 0.01)\n",
    "threshold = 0.2\n",
    "selected_features = info_gain_df[info_gain_df[\"Information Gain\"] > threshold][\"Feature\"].tolist()\n",
    "\n",
    "# Print selected features\n",
    "print(\"\\nSelected Features based on Information Gain:\")\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pearson’s Correlation Matrix\n",
    "A Pearson’s Correlation Matrix is a table that shows the Pearson correlation coefficients between multiple variables. The Pearson correlation coefficient (r) measures the linear relationship between two numerical variables, ranging from -1 to 1:\n",
    "\n",
    "r = 1 → Perfect positive correlation\n",
    "r = -1 → Perfect negative correlation\n",
    "r = 0 → No correlation\n",
    "The matrix helps identify relationships between features, aiding in feature selection by removing highly correlated (redundant) variables in machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute Pearson's correlation matrix\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(\"Pearson's Correlation Matrix:\")\n",
    "print(correlation_matrix.to_string())\n",
    "\n",
    "# Pearson's Correlation Matrix for feature selection\n",
    "correlation_matrix = df[numerical_features + categorical_features].corr().abs()\n",
    "\n",
    "# Define a threshold for high correlation\n",
    "correlation_threshold = 0.8\n",
    "high_correlation_features = set()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if correlation_matrix.iloc[i, j] > correlation_threshold:\n",
    "            high_correlation_features.add(correlation_matrix.columns[i])\n",
    "\n",
    "# Selecting features that are not highly correlated\n",
    "selected_features_pearson = [feature for feature in (numerical_features + categorical_features) if feature not in high_correlation_features]\n",
    "\n",
    "# Print selected features\n",
    "print(\"\\nSelected Features based on Pearson's Correlation:\")\n",
    "print(selected_features_pearson)\n",
    "\n",
    "# Visualize Pearson's Correlation Matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Pearson's Correlation Matrix Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Feature Selection Method\n",
    "Random Forest, an ensemble learning method, provides an effective way to select important features based on how much they contribute to decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df[\"NObeyesdad_Label\"]  # Target variable\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": rf_model.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importance from Random Forest:\")\n",
    "print(feature_importances.to_string(index=False))\n",
    "\n",
    "# Select top features based on a threshold\n",
    "importance_threshold = 0.01  # Example threshold\n",
    "selected_features_rf = feature_importances[feature_importances[\"Importance\"] > importance_threshold][\"Feature\"].tolist()\n",
    "\n",
    "print(\"\\nSelected Features based on Random Forest Importance:\")\n",
    "print(selected_features_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df[\"NObeyesdad_Label\"]  # Target variable\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Perform permutation importance\n",
    "perm_importance = permutation_importance(rf_model, X, y, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "\n",
    "# Store feature importance scores\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": perm_importance.importances_mean\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Permutation Feature Importance:\")\n",
    "print(feature_importance_df.to_string(index=False))\n",
    "\n",
    "# Select top features based on a threshold\n",
    "importance_threshold = 0.01  # Example threshold\n",
    "selected_features_permutation = feature_importance_df[feature_importance_df[\"Importance\"] > importance_threshold][\"Feature\"].tolist()\n",
    "\n",
    "print(\"\\nSelected Features based on Permutation Feature Importance:\")\n",
    "print(selected_features_permutation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Selection (Wrapper Method)  \n",
    "Forward Selection is a stepwise feature selection technique that starts with no features and iteratively adds the most important one based on model performance. The process stops when adding more features no longer improves accuracy. It helps in reducing complexity while maintaining model efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df[\"NObeyesdad_Label\"]  # Target variable\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Perform forward selection using Sequential Feature Selector\n",
    "sfs = SFS(logreg, \n",
    "          k_features=\"best\",  # Automatically selects the best number of features\n",
    "          forward=True,  # Forward selection\n",
    "          floating=False, \n",
    "          scoring=\"accuracy\", \n",
    "          cv=5)  # 5-fold cross-validation\n",
    "\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected_features_forward_wrapper = list(sfs.k_feature_names_)\n",
    "\n",
    "print(\"\\nSelected Features using Forward Selection (Wrapper Method):\")\n",
    "print(selected_features_forward_wrapper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward Selection (Wrapper Method)\n",
    "Backward Selection starts with all features and iteratively removes the least important one based on model performance. The process continues until removing features no longer improves accuracy. It helps in eliminating irrelevant features while maintaining model efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df[\"NObeyesdad_Label\"]  # Target variable\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Perform backward selection using Sequential Feature Selector\n",
    "sfs = SFS(logreg, \n",
    "          k_features=\"best\", \n",
    "          forward=False,  # Backward selection\n",
    "          floating=False, \n",
    "          scoring=\"accuracy\", \n",
    "          cv=5)  # 5-fold cross-validation\n",
    "\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Selected features\n",
    "selected_features_backward = list(sfs.k_feature_names_)\n",
    "\n",
    "print(\"\\nSelected Features using Backward Selection:\")\n",
    "print(selected_features_backward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (RFE)\n",
    "RFE is a wrapper method that iteratively removes the least important features based on model performance. It starts with all features, trains the model, and removes the weakest one until the optimal subset is found. This helps in selecting the most relevant features while reducing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[numerical_features + categorical_features]\n",
    "y = df[\"NObeyesdad_Label\"]  # Target variable\n",
    "\n",
    "# Apply Random Fourier Transform\n",
    "rft = RBFSampler(gamma=1.0, random_state=42, n_components=100)\n",
    "\n",
    "# Define a pipeline with RFT and Logistic Regression\n",
    "pipeline = Pipeline([\n",
    "    (\"rft\", rft),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Extract transformed features\n",
    "X_transformed = pipeline.named_steps[\"rft\"].transform(X)\n",
    "\n",
    "# Display shape of transformed feature set\n",
    "print(\"Shape of Transformed Feature Set using RFT:\", X_transformed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming the last column is the target variable\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[target])\n\u001b[0;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming the last column is the target variable\n",
    "target = df.columns[-1]\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Add a constant column for the intercept\n",
    "X = sm.add_constant(X)\n",
    "a\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Forward Selection\n",
    "selected_features = ['const']  # Start with only the intercept\n",
    "remaining_features = list(X_train.columns)\n",
    "remaining_features.remove('const')\n",
    "\n",
    "best_model = None\n",
    "best_adj_r2 = -float('inf')\n",
    "\n",
    "while remaining_features:\n",
    "    best_feature = None\n",
    "    for feature in remaining_features:\n",
    "        model = sm.OLS(y_train, X_train[selected_features + [feature]]).fit()\n",
    "        if model.rsquared_adj > best_adj_r2:\n",
    "            best_adj_r2 = model.rsquared_adj\n",
    "            best_feature = feature\n",
    "            best_model = model\n",
    "    \n",
    "    if best_feature:\n",
    "        selected_features.append(best_feature)\n",
    "        remaining_features.remove(best_feature)\n",
    "    else:\n",
    "        break  # Stop if no improvement\n",
    "\n",
    "print(\"Selected Features:\", selected_features)\n",
    "print(best_model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
